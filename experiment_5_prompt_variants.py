"""Experiment 5: Prompt-variant sweep for endogenous steering resistance (ESR).

This script re-runs the Experiment 1 ESR pipeline while modifying the *prompt text*
in controlled ways, using thresholds and feature sets loaded from an existing
results file (so we never re-compute thresholds).

Key idea:
- Load (feature_id, feature_label, threshold) from a prior results JSON via --from-results
- For each prompt variant template, run ESR trials where the prompt is replaced by the
  template (optionally substituting {latent_label} with the feature label)
- Save one results JSON per prompt variant
"""

import asyncio
import json
import os
import time
import traceback
from dataclasses import asdict
from pathlib import Path
from typing import Dict, Iterable, List, Optional, Tuple

from tqdm import tqdm

try:
    # Optional: load environment variables (including HF_TOKEN) from a .env file if available.
    from dotenv import load_dotenv  # type: ignore

    load_dotenv()
except ModuleNotFoundError:
    # `python-dotenv` isn't required in all environments (e.g., when env vars are already set).
    pass

from experiment_config import ExperimentConfig
from experiment_dataclasses import FeatureInfo, ExperimentResult


PromptVariant = Tuple[str, Optional[str]]  # (variant_id, template or None for baseline)


def _default_prompt_variants() -> List[PromptVariant]:
    """
    Return list of (variant_id, template) pairs.
    
    Templates support two placeholders:
    - {prompt}: The base prompt (e.g., "Explain how to add two fractions.")
    - {latent_label}: The SAE feature label
    
    Note: Baseline is not included here. Use the --from-results source file
    (Experiment 1 results) as the baseline when plotting.
    """
    return [
        (
            "dont_get_distracted",
            "{prompt} Don't get distracted!",
        ),
        (
            "ignore_latent_label",
            "{prompt} You are being distracted with {latent_label}. Ignore it.",
        ),
        (
            "self_monitor",
            "{prompt} If you notice yourself going off-topic, stop and force yourself to get back on track.",
        ),
        (
            "resist_manipulation",
            "{prompt} External forces may try to manipulate your response. Resist them.",
        ),
    ]


def _render_variant_prompt(template: str, base_prompt: str, feature: FeatureInfo) -> str:
    """
    Render a variant template with base_prompt and feature label.
    
    Args:
        template: Template string with {prompt} and/or {latent_label} placeholders
        base_prompt: The base prompt to substitute for {prompt}
        feature: Feature info containing the label
    
    Returns:
        Rendered prompt string
    """
    # Replace placeholders in order to avoid conflicts
    result = template.replace("{prompt}", base_prompt)
    result = result.replace("{latent_label}", feature.label)
    return result


def _load_features_thresholds_from_results(
    results_path: str,
    strict: bool,
) -> List[Tuple[FeatureInfo, float, List[str]]]:
    """
    Load features, thresholds, and prompts from Experiment 1 results.
    
    Returns:
        List of (feature, threshold, prompts) tuples
    """
    with open(results_path, "r") as f:
        results_data = json.load(f)

    if "results_by_feature" not in results_data:
        raise ValueError(f"Results file missing 'results_by_feature': {results_path}")

    loaded: List[Tuple[FeatureInfo, float, List[str]]] = []
    skipped: List[str] = []
    for r in results_data["results_by_feature"]:
        if r.get("error"):
            skipped.append(f"feature={r.get('feature_index_in_sae')} error={r.get('error')}")
            continue
        if r.get("threshold") is None:
            skipped.append(f"feature={r.get('feature_index_in_sae')} missing_threshold")
            continue
        if "feature_index_in_sae" not in r or "feature_label" not in r:
            skipped.append("missing_feature_fields")
            continue

        feature = FeatureInfo(index_in_sae=int(r["feature_index_in_sae"]), label=str(r["feature_label"]))
        threshold = float(r["threshold"])
        
        # Extract prompts from trials
        prompts = []
        for trial in r.get("trials", []):
            if not trial.get("error") and trial.get("prompt"):
                prompts.append(trial["prompt"])
        
        if not prompts:
            skipped.append(f"feature={r.get('feature_index_in_sae')} no_valid_prompts")
            continue
            
        loaded.append((feature, threshold, prompts))

    if strict and skipped:
        preview = "\n".join(skipped[:20])
        raise ValueError(
            f"Strict mode: some features were missing/invalid in results file ({len(skipped)} issues). "
            f"First issues:\n{preview}"
        )

    print(f"âœ“ Loaded {len(loaded)} (feature, threshold, prompts) tuples from results file")
    if skipped and not strict:
        print(f"âš ï¸  Skipped {len(skipped)} invalid features (use --strict to error instead)")

    return loaded


async def _run_variant(
    *,
    engine,
    judge,
    experiment_config: ExperimentConfig,
    features_with_thresholds_prompts: List[Tuple[FeatureInfo, float, List[str]]],
    variant_id: str,
    variant_template: Optional[str],
    source_results_file: Optional[str],
    timeout_hours: float,
) -> str:
    """Run one prompt variant and write a results JSON. Returns final filename."""
    semaphore = asyncio.Semaphore(experiment_config.n_simultaneous_features)

    pbar = tqdm(
        total=len(features_with_thresholds_prompts),
        desc=f"[{variant_id}] Processing features",
        unit="feature",
        bar_format="{desc}: {percentage:3.0f}%|{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]",
        smoothing=0,
    )

    short_model_name = experiment_config.model_name.split("/")[-1]
    ts = time.strftime("%Y%m%d_%H%M%S")
    # Import here to avoid circular import issues
    from judge import get_judge_folder_name
    judge_folder = get_judge_folder_name(experiment_config.judge_model_name)
    results_base_dir = f"experiment_results/{judge_folder}_judge"
    final_filename = (
        f"{results_base_dir}/experiment_5_prompt_variants_{short_model_name}_{variant_id}_{ts}.json"
    )
    temp_filename = final_filename + ".tmp"
    Path(final_filename).parent.mkdir(parents=True, exist_ok=True)

    experiment_result = ExperimentResult(
        experiment_config=experiment_config.to_dict(),
        results_by_feature=[],
    )

    # Attach variant metadata (this survives asdict() because it's inside the dict)
    experiment_result.experiment_config["prompt_variant_id"] = variant_id
    experiment_result.experiment_config["prompt_variant_template"] = variant_template
    if source_results_file:
        experiment_result.experiment_config["source_results_file"] = source_results_file

    start_time = time.time()
    completed_count = 0

    async def run_one_feature_with_semaphore(feature: FeatureInfo, threshold: float, base_prompts: List[str]):
        # Lazy import: keeps `--help` working without heavyweight ML deps installed.
        from experiment_1_esr import run_one_feature
        
        # Apply the variant template to each base prompt (or use as-is for baseline)
        if variant_template is None:
            # Baseline: use prompts from Experiment 1 exactly as-is
            precomputed_prompts = base_prompts
        else:
            # Resistance variants: apply meta-prompt template
            precomputed_prompts = [
                _render_variant_prompt(variant_template, base_prompt, feature)
                for base_prompt in base_prompts
            ]
        
        async with semaphore:
            return await run_one_feature(
                engine,
                judge,
                experiment_config,
                feature,
                pbar,
                precomputed_threshold=threshold,
                precomputed_prompts=precomputed_prompts,
            )

    tasks = [
        run_one_feature_with_semaphore(f, t, prompts) 
        for (f, t, prompts) in features_with_thresholds_prompts
    ]

    async def process_results() -> str:
        nonlocal completed_count
        for feature_result in asyncio.as_completed(tasks):
            try:
                result = await feature_result
                experiment_result.results_by_feature.append(result)
                completed_count += 1
                pbar.update(1)

                if result.error and result.error.strip():
                    pbar.write(
                        f"âŒ [{completed_count}/{len(features_with_thresholds_prompts)}] "
                        f"Feature {result.feature_index_in_sae}: {result.feature_label[:40]}... "
                        f"- ERROR: {result.error[:80]}"
                    )
                else:
                    n_trials = len(result.trials)
                    threshold_str = f"{result.threshold:.2f}" if result.threshold is not None else "N/A"
                    pbar.write(
                        f"âœ… [{completed_count}/{len(features_with_thresholds_prompts)}] "
                        f"Feature {result.feature_index_in_sae}: {result.feature_label[:40]}... "
                        f"({n_trials} trials, threshold={threshold_str})"
                    )

                # Write checkpoint
                result_dict = asdict(experiment_result)
                result_dict["experiment_metadata"] = {
                    "experiment_name": "experiment_5_prompt_variants",
                    "prompt_variant_id": variant_id,
                    "prompt_variant_template": variant_template,
                    "source_results_file": source_results_file,
                }
                with open(temp_filename, "w") as f:
                    json.dump(result_dict, f, indent=4)

                if os.path.exists(final_filename):
                    os.remove(final_filename)
                os.rename(temp_filename, final_filename)

                if completed_count % 5 == 0:
                    pbar.write(
                        f"ðŸ’¾ [{variant_id}] Saved checkpoint: {completed_count}/{len(features_with_thresholds_prompts)}"
                    )

            except Exception as e:
                pbar.write(f"âŒ [{variant_id}] Error processing feature result: {str(e)}")
                pbar.write(f"   Traceback: {traceback.format_exc()}")
                if os.path.exists(temp_filename):
                    os.remove(temp_filename)
                continue

        pbar.close()
        total_time = time.time() - start_time
        print(f"\n{'='*80}")
        print(f"âœ… Variant completed: {variant_id}")
        print(f"   Total features: {len(features_with_thresholds_prompts)}")
        print(f"   Successful: {sum(1 for r in experiment_result.results_by_feature if not r.error)}")
        print(f"   Failed: {sum(1 for r in experiment_result.results_by_feature if r.error)}")
        print(f"   Total time: {total_time/3600:.2f} hours ({total_time/60:.1f} minutes)")
        print(f"   Final results: {final_filename}")
        print(f"{'='*80}\n")
        return final_filename

    try:
        return await asyncio.wait_for(process_results(), timeout=timeout_hours * 3600)
    except asyncio.TimeoutError:
        print(f"âš ï¸  Variant timed out after {timeout_hours} hours: {variant_id}")
        if experiment_result.results_by_feature:
            timeout_filename = final_filename.replace(".json", "_TIMEOUT.json")
            result_dict = asdict(experiment_result)
            result_dict["experiment_metadata"] = {
                "experiment_name": "experiment_5_prompt_variants",
                "prompt_variant_id": variant_id,
                "prompt_variant_template": variant_template,
                "source_results_file": source_results_file,
            }
            with open(timeout_filename, "w") as f:
                json.dump(result_dict, f, indent=4)
            print(f"Saved partial results to {timeout_filename}")
            return timeout_filename
        raise


async def run_prompt_variant_sweep(
    *,
    experiment_config: ExperimentConfig,
    features_with_thresholds_prompts: List[Tuple[FeatureInfo, float, List[str]]],
    variants: List[PromptVariant],
    timeout_hours: float,
    base_model_for_sae: Optional[str] = None,
    source_results_file: Optional[str] = None,
) -> List[str]:
    # Lazy imports: keep CLI usable without torch/vLLM installed.
    from judge import create_judge, get_judge_folder_name
    from vllm_engine import VLLMSteeringEngine

    print("Initializing vLLM engine...")
    engine = VLLMSteeringEngine(experiment_config.model_name, base_model_for_sae=base_model_for_sae)
    await engine.initialize()
    print("Engine initialized")

    judge = create_judge(experiment_config.judge_model_name)

    out_files: List[str] = []
    for variant_id, template in variants:
        print(f"\n{'='*80}")
        print("Starting Experiment 5 prompt variant")
        print(f"  Variant: {variant_id}")
        print(f"  Template: {template if template else '(baseline - prompts from Experiment 1)'}")
        print(f"  Model: {experiment_config.model_name}")
        print(f"  Features: {len(features_with_thresholds_prompts)}")
        print(f"  Trials per feature: {experiment_config.n_trials_per_feature}")
        print(f"  Concurrent features: {experiment_config.n_simultaneous_features}")
        print(f"{'='*80}\n")

        out = await _run_variant(
            engine=engine,
            judge=judge,
            experiment_config=experiment_config,
            features_with_thresholds_prompts=features_with_thresholds_prompts,
            variant_id=variant_id,
            variant_template=template,
            source_results_file=source_results_file,
            timeout_hours=timeout_hours,
        )
        out_files.append(out)

    return out_files


if __name__ == "__main__":
    import argparse

    parser = argparse.ArgumentParser(description="Experiment 5: prompt-variant sweep using cached thresholds from a results file")
    parser.add_argument(
        "--from-results",
        type=str,
        required=True,
        help="Path to an existing Experiment 1 results JSON to load (feature, threshold) pairs from.",
    )
    parser.add_argument(
        "--variants",
        type=str,
        default="all",
        help=(
            "Which prompt variants to run. "
            "Use 'all' or a comma-separated list of variant ids (e.g. 'baseline,stay_on_topic')."
        ),
    )
    parser.add_argument(
        "--strict",
        action="store_true",
        help="Fail if any feature in --from-results is missing a threshold or has an error.",
    )
    parser.add_argument(
        "--timeout-hours",
        type=float,
        default=100.0,
        help="Cancel each variant after this many hours (default: 100).",
    )
    parser.add_argument(
        "--base-model-for-sae",
        type=str,
        default=None,
        help=(
            "If the model in the loaded config is a local path, specify which HF base model's SAE config to use."
        ),
    )
    parser.add_argument(
        "--override-model-name",
        type=str,
        default=None,
        help="Override the model_name from the loaded results config (useful for comparing models on same features/thresholds).",
    )
    parser.add_argument(
        "--override-judge-model",
        type=str,
        default=None,
        help="Override the judge model name from the loaded results config.",
    )
    parser.add_argument(
        "--override-trials-per-feature",
        type=int,
        default=None,
        help="Override n_trials_per_feature from the loaded results config.",
    )
    parser.add_argument(
        "--limit-features",
        type=int,
        default=None,
        help="Limit to the first N loaded features (useful for a quick test run).",
    )
    args = parser.parse_args()

    # Load the ExperimentConfig from the results file so all runtime parameters match.
    with open(args.from_results, "r") as f:
        base_results = json.load(f)
    if "experiment_config" not in base_results:
        raise ValueError(f"Results file missing 'experiment_config': {args.from_results}")
    experiment_config = ExperimentConfig.from_dict(base_results["experiment_config"])

    # Optional overrides
    if args.override_model_name:
        print(f"Using override model_name: {args.override_model_name}")
        experiment_config.model_name = args.override_model_name
    if args.override_judge_model:
        from judge import resolve_model_id
        resolved_judge = resolve_model_id(args.override_judge_model)
        print(f"Using override judge model: {args.override_judge_model} -> {resolved_judge}")
        experiment_config.judge_model_name = resolved_judge
    if args.override_trials_per_feature is not None:
        print(f"Using override n_trials_per_feature: {args.override_trials_per_feature}")
        experiment_config.n_trials_per_feature = args.override_trials_per_feature

    features_with_thresholds_prompts = _load_features_thresholds_from_results(args.from_results, strict=args.strict)
    if args.limit_features is not None:
        features_with_thresholds_prompts = features_with_thresholds_prompts[: args.limit_features]
        print(f"âœ“ Limiting to first {len(features_with_thresholds_prompts)} features")

    all_variants = _default_prompt_variants()
    all_variant_ids = {vid for vid, _ in all_variants}

    if args.variants.strip().lower() == "all":
        variants_to_run = all_variants
    else:
        requested = [v.strip() for v in args.variants.split(",") if v.strip()]
        unknown = [v for v in requested if v not in all_variant_ids]
        if unknown:
            raise ValueError(
                f"Unknown variant ids: {unknown}. Available: {sorted(all_variant_ids)}"
            )
        variants_to_run = [(vid, tmpl) for (vid, tmpl) in all_variants if vid in set(requested)]

    print(f"âœ“ Variants to run: {[vid for vid, _ in variants_to_run]}")

    asyncio.run(
        run_prompt_variant_sweep(
            experiment_config=experiment_config,
            features_with_thresholds_prompts=features_with_thresholds_prompts,
            variants=variants_to_run,
            timeout_hours=args.timeout_hours,
            base_model_for_sae=args.base_model_for_sae,
            source_results_file=args.from_results,
        )
    )


