# ESR Paper to Code Mapping

This document maps each section and figure in the ICML 2026 ESR paper to the corresponding code and data files.

## Main Text Figures

### Figure 1: ESR Demonstration Example
**Paper Location**: Section 1 (Introduction), Figure 1
**Description**: Example of Llama-3.3-70B correcting from "body positions" steering back to probability question
**Generated by**: Manual selection from Experiment 1 results
**Data**: `experiment_results/experiment_results_Meta-Llama-3.3-70B-Instruct_*.json`
**Visualization**: Manual LaTeX formatting with `attemptbox` environments

### Figure 2: ESR Across Models
**Paper Location**: Section 3.1, Figure 2 (3-panel figure)
**Description**: Score delta histograms, multi-attempt rates, and MSI across 5 models
**Script**: `experiment_1_esr.py`
**Plotting**: `plotting/plot_exp1.py`
**Expected Files**: `plots/experiment_1_esr_combined_figure.png`

### Figure 3: Boost Level Ablation
**Paper Location**: Section 3.2, Figure 3
**Description**: ESR metrics vs. boost relative to threshold for Llama-3.3-70B
**Script**: `experiment_2_multi_boost.py`
**Plotting**: `plotting/plot_exp2.py`
**Expected Files**: `plots/experiment_2_multi_boost_Llama_3.3_70B.png`

### Figure 4: Meta-Prompting Enhancement
**Paper Location**: Section 3.3, Figure 4 (3-panel figure)
**Description**: Baseline vs. self-monitor meta-prompt across all metrics and models
**Script**: `experiment_5_prompt_variants.py`
**Plotting**: `plotting/plot_exp5.py`
**Expected Files**: `plots/experiment_5_combined_baseline_baseline_vs_self_monitor.png`

### Figure 5: Off-Topic Detector Ablation
**Paper Location**: Section 3.4, Figure 5 (3-panel bar chart)
**Description**: ESR metrics with and without OTD ablation
**Script**: `experiment_3_off_topic_detectors/experiment_3_with_ablation.py`
**Plotting**: `plotting/plot_exp3.py`
**Expected Files**: `plots/experiment_3_ablation_metrics_bar_chart_combined_25latents.png`

### Figure 6: Sequential Activations During Self-Correction
**Paper Location**: Section 3.4, Figure 6
**Description**: Token-level activation traces of OTD and backtracking latents
**Script**: `appendices/self-correction-activation-statistics/plot_activations.py`
**Data**: Self-correction episodes extracted from Experiment 1 results
**Expected Files**: `plots/experiment_6_sequential_activations.png`

### Figure 7: Fine-Tuning Results
**Paper Location**: Section 3.5, Figure 7 (3-panel figure)
**Description**: Mean attempts, success rate, and MSI vs. training data ratio
**Script**: `experiment_4_finetuning/run_esr.sh` (evaluation after training)
**Plotting**: `plotting/plot_exp4.py`
**Expected Files**: `plots/experiment_4_masked_ratio_sweep.png`

## Appendix Figures

### Appendix Figure: No-Steering Baseline
**Paper Location**: Appendix A.3.1
**Description**: Multi-attempt rate and scores without steering interventions
**Script**: Experiment 1 with `--no-steering` flag
**Plotting**: `plotting/plot_exp8.py`
**Expected Files**: `plots/experiment_8_baseline_*.png`

### Appendix Figure: ESR Including Degraded Outputs
**Paper Location**: Appendix A.1.4
**Description**: Robustness check showing results with degraded outputs included
**Script**: Experiment 1 results analyzed without degradation filtering
**Plotting**: `plotting/plot_exp1_include_degraded.py`
**Expected Files**: `plots/experiment_1_esr_combined_figure_include_degraded.png`

### Appendix Figure: Cross-Judge Validation
**Paper Location**: Appendix A.2.2
**Description**: MSI, ESR rate, and attempt count agreement across 5 judge models
**Script**: `regrade_cross_judge.py`
**Plotting**: `plotting/plot_exp7.py`
**Expected Files**:
- `plots/experiment_7_cross_judge_msi.png`
- `plots/experiment_7_cross_judge_esr_rate.png`
- `plots/experiment_7_cross_judge_attempts_scatter.png`

### Appendix Figure: Meta-Prompt Variant Comparison (Per-Model)
**Paper Location**: Appendix A.3.4
**Description**: Bar charts comparing all meta-prompt variants for each model
**Script**: `experiment_5_prompt_variants.py` (all variants)
**Plotting**: `plotting/plot_exp5.py` (individual model breakdowns)
**Expected Files**: `plots/experiment_5_prompt_variant_bars_<model>.png` (5 files)

### Appendix Figure: Random Ablation Control
**Paper Location**: Appendix A.3.6
**Description**: Comparison of OTD ablation vs. random latent ablation vs. baseline
**Script**: `random_ablation_control/run_ablation_experiment.py`
**Plotting**: `random_ablation_control/create_plot.py`
**Expected Files**: `plots/random_ablation_control_comparison.png`

### Appendix Figure: Token-Level Activations with Baseline Comparison
**Paper Location**: Appendix A.4.1
**Description**: OTD activation aligned at correction point and compared to baseline
**Script**: `appendices/self-correction-activation-statistics/analyze_activations.py`
**Plotting**: `appendices/self-correction-activation-statistics/plot_activations.py`
**Expected Files**:
- `plots/app_otd_seq_aligned_overlay_spline.png`
- `plots/app_otd_seq_baseline_comparison.png`

## Tables

### Table 1: Models and SAEs
**Paper Location**: Section 2.2, Table 1
**Data Source**: `experiment_config.py` (MODEL_CONFIGS)
**Content**: Model names, SAE types, steering layers, and relative depths

### Table 2: Degradation Rates by Model
**Paper Location**: Appendix A.1.4, Table 2
**Data Source**: Computed from Experiment 1 results
**Script**: `plotting/plot_exp1.py` (statistics computation)

### Table 3: No-Steering Baseline Results
**Paper Location**: Appendix A.3.1, Table 3
**Data Source**: Experiment 1 with `--no-steering` flag
**Content**: Trial counts, first-attempt scores, multi-attempt rates

### Table 4: Off-Topic Detector Latent Statistics
**Paper Location**: Appendix A.3.5, Table 4
**Data Source**: `data/off_topic_detectors_v2.json` or `data/off_topic_detectors_separability.json`
**Script**: `appendices/otd-activation-statistics/generate_otd_table.py`
**Content**: Latent IDs, labels, activation statistics for shuffled vs. normal pairs

### Table 5: Fine-Tuning Hyperparameters
**Paper Location**: Appendix A.3.7, Table 5
**Data Source**: `experiment_4_finetuning/instruct-lora-8b-with-masking.yaml`
**Content**: LoRA config, learning rate, batch sizes, etc.

## Experiment-to-Paper Section Mapping

| Experiment | Paper Section | Description |
|------------|---------------|-------------|
| `experiment_1_esr.py` | §3.1 (Results) | ESR across model sizes |
| `experiment_2_multi_boost.py` | §3.2 (Results) | Boost level ablation |
| `experiment_3_off_topic_detectors/` | §3.4 (Results) | OTD identification and ablation |
| `experiment_4_finetuning/` | §3.5 (Results) | Fine-tuning for ESR induction |
| `experiment_5_prompt_variants.py` | §3.3 (Results) | Meta-prompting enhancement |
| `appendices/self-correction-activation-statistics/` | §3.6 (Results) + Appendix A.4 | Sequential activation analysis |
| `regrade_cross_judge.py` | Appendix A.2.2 | Judge model validation |
| Experiment 1 with `--no-steering` | Appendix A.3.1 | No-steering baseline control |
| `random_ablation_control/` | Appendix A.3.6 | Random latent ablation control |
| `appendices/otd-activation-statistics/` | Appendix A.3.5 | OTD latent table generation |

## Data Files and Their Purpose

| File | Purpose | Used By |
|------|---------|---------|
| `prompts.txt` | 38 object-level "explain how" prompts | All experiments |
| `data/off_topic_detectors_v2.json` | 25 OTD latents (original discovery) | Experiment 3 ablation |
| `data/off_topic_detectors_separability.json` | 25 OTD latents (separability-based) | Random ablation control |
| `data/normal_responses.json` | Unsteered baseline responses | OTD discovery, baseline comparisons |
| `experiment_4_finetuning/off_topic_subjects.txt` | 50 off-topic subjects for synthetic data | Fine-tuning data generation |
| `experiment_4_finetuning/dataset_normal_responses.json` | Normal response training data | Fine-tuning |
| `experiment_results/*.json` | Experiment outputs with trials, scores, attempts | Analysis and plotting |

## Reproducing Specific Paper Claims

### "Llama-3.3-70B shows 1.7% multi-attempt rate" (§3.1)
**Run**: `python experiment_1_esr.py 70b`
**Metric**: Count responses with >1 attempt / total responses
**Expected**: ~1.7% (may vary slightly with different latent samples)

### "Ablating OTDs reduces ESR by 54%" (§3.4)
**Run**:
```bash
python experiment_3_off_topic_detectors/experiment_3_with_ablation.py 70b --ablate data/off_topic_detectors_v2.json
```
**Compare**: MSI with vs. without ablation
**Expected**: ~54% reduction (0.53 → 0.24)

### "Meta-prompt increases MSI by 5× for Llama-3.3-70B" (§3.3)
**Run**: `python experiment_5_prompt_variants.py 70b`
**Compare**: MSI for baseline vs. "self_monitor" prompt variant
**Expected**: ~5× increase (0.55 → 2.94)

### "OTDs fire 7.7× higher during off-topic content" (§3.6)
**Script**: `appendices/self-correction-activation-statistics/analyze_activations.py`
**Data**: Self-correction episodes from Experiment 1
**Expected**: Mean OTD activation ratio ~7.7× baseline

### "Fine-tuned 8B achieves MSI = 3.4 at 50% training data" (§3.5)
**Run**: Full fine-tuning pipeline in `experiment_4_finetuning/`
**Model**: Checkpoint trained on 50% self-correction data
**Expected**: MSI ~3.4 on steered evaluation

### "0% multi-attempt responses without steering" (Appendix A.3.1)
**Run**: Experiment 1 with `--no-steering` flag
**Expected**: Exactly 0 multi-attempt responses across all trials

## Judge Prompts

The judge model prompts used for scoring are documented in:
- **Main judge prompt**: Appendix A.2.1 (§"Judge Prompt")
- **Concreteness judge**: Appendix A.2.1 (§"Concreteness Judge")

These prompts are implemented in `claude_judge.py`.

## Notes on Reproducibility

1. **Random Seeds**: Experiments use random seeds for latent sampling and trial selection. Exact numerical values may vary slightly, but trends and significance should be consistent.

2. **Model Versions**: Paper uses specific model versions (Llama-3.3-70B-Instruct, Llama-3.1-8B-Instruct, Gemma-2-*-it). Results may differ with updated model releases.

3. **SAE Versions**: Goodfire and GemmaScope SAEs are versioned. The paper uses SAEs available as of October 2025. Future SAE versions may produce different steering effects.

4. **Degradation Filtering**: By default, outputs with ≥5 consecutive repeated words are filtered. This is documented in Appendix A.1.4. To include degraded outputs, modify filtering logic in experiment scripts.

5. **Threshold Calibration**: Each latent's threshold is calibrated to achieve average first-attempt score of 50/100. This process is stochastic (Probabilistic Bisection Algorithm) and may require 5-15 trials per latent.

6. **Computational Resources**: Full reproduction requires:
   - GPU: A100 (80GB) or H100 recommended
   - Total GPU hours: ~185-245 hours (see README.md)
   - Estimated cost: $2,000-$3,000 on cloud providers

## Contact for Questions

For questions about reproducing specific results or interpreting code, contact:
- Alex McKenzie (alex.mckenzie@ae.studio)
- See paper for full author list
